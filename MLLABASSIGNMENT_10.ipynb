{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLLABASSIGNMENT_10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9ZQjO2X4KOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzmyZmI17J1T",
        "colab_type": "text"
      },
      "source": [
        "problem 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiNUuM3Z4hUn",
        "colab_type": "code",
        "outputId": "e7043113-3852-4929-b8c9-fc82190ab3da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "massDist = (0.2,0.4,0.1,0.1,0.1,0.1)\n",
        "\n",
        "np.random.seed()\n",
        "\n",
        "\n",
        "for i in range(250):\n",
        "    randwalk = [0]\n",
        "    for x in range(100):\n",
        "        step = randwalk[-1]\n",
        "        dice = np.random.randint(1,7)\n",
        "        if dice <= 2 :\n",
        "            step = max(0, step - 1)\n",
        "\n",
        "        elif dice<=5:\n",
        "            step += 1\n",
        "\n",
        "        else:\n",
        "            step = step + np.random.randint(1,7)\n",
        "        \n",
        "    print(step)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "6\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "4\n",
            "0\n",
            "6\n",
            "0\n",
            "1\n",
            "1\n",
            "3\n",
            "0\n",
            "0\n",
            "1\n",
            "5\n",
            "4\n",
            "6\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "5\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "6\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "6\n",
            "0\n",
            "6\n",
            "1\n",
            "6\n",
            "1\n",
            "0\n",
            "1\n",
            "5\n",
            "1\n",
            "1\n",
            "1\n",
            "6\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "2\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "6\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "6\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "1\n",
            "0\n",
            "3\n",
            "1\n",
            "0\n",
            "6\n",
            "0\n",
            "3\n",
            "0\n",
            "1\n",
            "6\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "6\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "2\n",
            "1\n",
            "5\n",
            "1\n",
            "5\n",
            "0\n",
            "1\n",
            "1\n",
            "5\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "6\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyCjsNQI5kSY",
        "colab_type": "code",
        "outputId": "a5828a69-2df6-4196-a5ec-f534bddefd30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def roll(massDist):\n",
        "    randRoll = random.random() # in [0,1]\n",
        "    sum = 0\n",
        "    result = 1\n",
        "    for mass in massDist:\n",
        "        sum += mass\n",
        "        if randRoll < sum:\n",
        "            return result\n",
        "        result+=1\n",
        "\n",
        "def dice(massDict):\n",
        "  step = 0;\n",
        "  numstep = 0;\n",
        "  for i in range(100000):\n",
        "    for i in range(250):\n",
        "      result = roll(massDict)\n",
        "      if result==1 or result==2: step=max(0,step-1)\n",
        "      elif result>=3 and result<=5: step=step+1\n",
        "      else: \n",
        "        result1 = roll(massDict)\n",
        "        step=step+result1\n",
        "    if step>60: numstep=numstep+1\n",
        "  return numstep\n",
        "numstep = dice(massDist)\n",
        "print(\"Probability = \",numstep/100000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability =  0.28628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilj6j2H77PyQ",
        "colab_type": "text"
      },
      "source": [
        "Random data for multilinear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a1ppo_c6Ud-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "bc2931c7-c266-4d52-eaed-6eb1c7c8daf3"
      },
      "source": [
        "import scipy\n",
        "from scipy.stats import norm\n",
        "random.seed(1)\n",
        "n_features = 4\n",
        "X = []\n",
        "for i in range(n_features):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "#print(X)\n",
        "eps = scipy.stats.norm.rvs(0, 0.25,100)\n",
        "y = 1 + (0.5 * X[0]) + eps + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3])\n",
        "data_mlr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y }\n",
        "df = pd.DataFrame(data_mlr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0 -1.435440  2.129929 -0.263304  0.958279  1.147244\n",
            "1  0.088946 -0.489570  1.393320 -0.503175  1.415012\n",
            "2 -0.681597 -0.208987 -2.188540 -1.244343 -0.910826\n",
            "3  0.063020  1.708245  0.579512 -1.365972  0.989570\n",
            "4 -0.626231  0.992590  1.611015  0.415867  1.698834\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.888984 -2.340832 -0.413660 -0.315739 -0.576890\n",
            "96 -0.096416 -1.994776  0.150179 -0.640341 -0.510997\n",
            "97  0.252935 -2.364046 -0.509880 -0.704769 -0.036079\n",
            "98 -0.330634 -0.518680 -0.342886 -0.948565  0.035658\n",
            "99  1.947872  0.221277 -0.600942  0.374685  2.630138\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.096001   -0.135420    0.108492   -0.101159    0.981336\n",
            "std      0.923482    1.126042    0.974667    0.894605    0.986679\n",
            "min     -2.533493   -3.656007   -2.564621   -2.034291   -1.417669\n",
            "25%     -0.369427   -0.813279   -0.606671   -0.648360    0.218006\n",
            "50%      0.154333   -0.081358    0.165226   -0.251989    0.992066\n",
            "75%      0.597452    0.511822    0.795880    0.511589    1.712349\n",
            "max      1.947872    2.198082    2.471684    2.178388    3.227079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMls7ez97Zw7",
        "colab_type": "text"
      },
      "source": [
        "random data for logisitc regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR_Gldw26sSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "9bc64192-03d0-42ee-db26-9f2ca62e4d5e"
      },
      "source": [
        "\n",
        "n_features = 4\n",
        "X = []\n",
        "for i in range(n_features):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "#print(X)\n",
        "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
        "#print(a1)\n",
        "y1 = []\n",
        "for i in a1:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "#print(y1)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
        "df1 = pd.DataFrame(data_lr)\n",
        "print(df1.head())\n",
        "print(df1.tail())\n",
        "print(df1.info())\n",
        "print(df1.describe())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3  Y\n",
            "0  1.008691 -0.405841  0.387917 -1.169090  1\n",
            "1 -0.259832  0.038056 -0.775247  0.524101  1\n",
            "2  0.884848  1.272891  0.513900 -0.197026  1\n",
            "3 -0.340371 -0.315624  0.701222 -0.289976  1\n",
            "4 -0.434794  1.359222 -1.498858  0.069014  1\n",
            "          X0        X1        X2        X3  Y\n",
            "95 -0.475614 -1.039287  0.074606 -0.235660  1\n",
            "96  0.608333  0.632609 -0.697918 -2.880363  0\n",
            "97  1.777407 -1.315167 -1.220985  1.278016  1\n",
            "98  0.093499  0.302989  0.784088 -0.716710  1\n",
            "99  0.412040  0.544952  1.503949 -0.019948  1\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    int64  \n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.078060   -0.046122   -0.191798    0.035983    0.890000\n",
            "std      0.987414    1.068792    1.134961    0.972215    0.314466\n",
            "min     -3.166260   -2.540917   -2.661551   -2.880363    0.000000\n",
            "25%     -0.491178   -0.718425   -0.964193   -0.614715    1.000000\n",
            "50%     -0.047484   -0.103739   -0.212102   -0.040028    1.000000\n",
            "75%      0.679370    0.689377    0.655938    0.559775    1.000000\n",
            "max      2.371695    2.376637    2.280718    2.487086    1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZBNLTnz7COn",
        "colab_type": "text"
      },
      "source": [
        "Random data for k clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogTCi4kA666S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "outputId": "f73654ba-1a39-4466-a57c-148307b25e6a"
      },
      "source": [
        "X_a= -2 * np.random.rand(100,2)\n",
        "X_b = 1 + 2 * np.random.rand(50,2)\n",
        "X_a[50:100, :] = X_b\n",
        "plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)\n",
        "plt.show()\n",
        "data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "df3 = pd.DataFrame(data_kmeans)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0 -1.435440  2.129929 -0.263304  0.958279  1.147244\n",
            "1  0.088946 -0.489570  1.393320 -0.503175  1.415012\n",
            "2 -0.681597 -0.208987 -2.188540 -1.244343 -0.910826\n",
            "3  0.063020  1.708245  0.579512 -1.365972  0.989570\n",
            "4 -0.626231  0.992590  1.611015  0.415867  1.698834\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.888984 -2.340832 -0.413660 -0.315739 -0.576890\n",
            "96 -0.096416 -1.994776  0.150179 -0.640341 -0.510997\n",
            "97  0.252935 -2.364046 -0.509880 -0.704769 -0.036079\n",
            "98 -0.330634 -0.518680 -0.342886 -0.948565  0.035658\n",
            "99  1.947872  0.221277 -0.600942  0.374685  2.630138\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.096001   -0.135420    0.108492   -0.101159    0.981336\n",
            "std      0.923482    1.126042    0.974667    0.894605    0.986679\n",
            "min     -2.533493   -3.656007   -2.564621   -2.034291   -1.417669\n",
            "25%     -0.369427   -0.813279   -0.606671   -0.648360    0.218006\n",
            "50%      0.154333   -0.081358    0.165226   -0.251989    0.992066\n",
            "75%      0.597452    0.511822    0.795880    0.511589    1.712349\n",
            "max      1.947872    2.198082    2.471684    2.178388    3.227079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqZynryb77y2",
        "colab_type": "text"
      },
      "source": [
        "PROBLEM 3\n",
        "Linear Regression using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma0DqPPk7v3e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fee57a3-4c90-483b-8e56-195139599056"
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - y_p))\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1153330657718508 0.17698551073763366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0M_CALC8XXn",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression using Gradient DescenT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPyJ69SJ8JC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6b04ad38-b7f8-4b50-9e9d-6928d2d6ba7b"
      },
      "source": [
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.346067807322643\n",
            "0.3456317253785163\n",
            "0.3452068771809113\n",
            "0.34479305392847864\n",
            "0.3443900501197875\n",
            "0.3439976631776432\n",
            "0.3436156931103449\n",
            "0.3432439422095357\n",
            "0.3428822147840344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKsc3HDB8kKU",
        "colab_type": "text"
      },
      "source": [
        "Linear regression using L1 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4dSgCIa8csy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9d57b18-53b9-4125-f7be-23fe2b3eaf74"
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10613228558882866 0.17706978660435213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98MpE7Gp8yPp",
        "colab_type": "text"
      },
      "source": [
        "Linear regression using L2\n",
        " regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAa_p_YY8pT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d938913c-2bd5-41a0-8eda-c3e1ddd46481"
      },
      "source": [
        "\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.20776675196921487 0.3200678911028933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF7bXCaz9bP5",
        "colab_type": "text"
      },
      "source": [
        "LOGISTIC regression using L1 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG0LXugQ9OMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "109d57c8-7fe2-4012-b38b-9980da158dfd"
      },
      "source": [
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "-0.05222439159509806\n",
            "-0.4467375458480385\n",
            "-0.8373021248631114\n",
            "-1.2239375638507695\n",
            "-1.6066648047229841\n",
            "-1.985506254395996\n",
            "-2.3604857394877334\n",
            "-2.7316284576935614\n",
            "-3.09896092614201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuypyqON9qCL",
        "colab_type": "text"
      },
      "source": [
        "LOGISTIC regression using L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3zSjluj9fx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a851b6a3-a8f0-4929-e625-1fcc39dd3da2"
      },
      "source": [
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.3465131734253519\n",
            "0.3473729520194667\n",
            "0.3490362962096079\n",
            "0.35144758022514855\n",
            "0.35455383624916\n",
            "0.3583046486412554\n",
            "0.3626520534299146\n",
            "0.3675504425537253\n",
            "0.37295647238492396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKWKMA2M9twK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#k clustering\n",
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzxqwDZ082OZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "66f5a04b-3959-4f67-fbeb-8def2a24a64c"
      },
      "source": [
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\".\", color=color, s=150, linewidths=5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DngHb8I9_WH3",
        "colab_type": "text"
      },
      "source": [
        "Problem Statement 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOCxrnbz-NH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Linear regression from scratch using oops\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.total_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9kWnEx7_fn6",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression from scratch using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng60f1pl-bgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olBbtSHV_l47",
        "colab_type": "text"
      },
      "source": [
        "K means clustering  from scratch using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7dLESLL-nC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}